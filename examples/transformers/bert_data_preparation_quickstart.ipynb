{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Prepare Data for BERT Model </h1>\n",
    "\n",
    "The example in this Jupyter notebook walks through the process of preparing data for a Bidirectional Transformers for Language Understanding (BERT) model.  Before working with this notebook, it is a good idea to review the **Input Representation** section of [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) (Devlin, Chang, Lee, and Toutanova) to understand the prerequisite requirements for data used by BERT models. \n",
    "\n",
    "This Jupyter notebook takes a step-by-step approach. \n",
    "\n",
    "First, prepare your computing environment and tools: \n",
    "\n",
    "* [Define and Configure the Computing Environment](#Define-and-Configure-the-Computing-Environment)\n",
    "* [Connect to the SAS Viya Server](#Connect-to-SAS-Viya-Server)\n",
    "* [Download the Raw Data](#Download-the-Data).\n",
    "\n",
    "Next, perform the data preparation task for the BERT model using three steps:\n",
    "\n",
    "1. [Explore and Clean the Data](#Data-Exploration-and-Cleaning)\n",
    "2. [Tokenize the Data](#Data-Tokenization)\n",
    "3. [Export the Data to a SAS Viya Table](#Export-Data-to-Viya-Table)\n",
    "\n",
    "The output of this example is an organized and cleaned data set suitable for use with a BERT model.\n",
    "\n",
    "\n",
    "<h2> Client and Server Terminology in this Example</h2>\n",
    "\n",
    "SAS Viya literature and technical documentation often refers to client and server entities. In this example, the client is the computer that runs the Jupyter notebook, and the server is the computer that runs the SAS Viya server. These two computers might (or might not) use the same operating system, and might (or might not) have access to a common file system.\n",
    "\n",
    "System configurations vary: the client and server operations in this example might be different in other environments. This example only requires that the data is accessible from the client computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Define-and-Configure-the-Computing-Environment\"></a>\n",
    "<h2> Define and Configure the Computing Environment </h2>\n",
    "\n",
    "The example defines and configures the computing requirement in the following steps: \n",
    "\n",
    "* [Import Required Python Packages](#Import-Required-Python-Packages)\n",
    "* [Specify Client Parameters](#Specify-Client-Parameters)\n",
    "* [Connect to the SAS Viya Server](#Connect-to-SAS-Viya-Server).\n",
    "\n",
    "\n",
    "<a id=\"Import-Required-Python-Packages\"></a>\n",
    "<h3> Import Required Python Function Packages </h3>\n",
    "\n",
    "This section of code imports the Python packages that contain the functions required to perform the BERT data preparation.\n",
    "\n",
    "**Note:** The SAS Deep Learning tools require the [Huggingface implementation](https://github.com/huggingface/transformers) of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "# Import sys and os packages\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Import Numpy scientific computing and \n",
    "# Pandas data structures libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import BERT Tokenizer function\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Specify-Client-Parameters\"></a>\n",
    "<h3> Specify Client Parameters </h3>\n",
    "\n",
    "This section of code configures the notebook for use with Linux or Windows SAS Viya clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalize the example Jupyter notebook so it can be used with Linux and Windows Viya clients\n",
    "\n",
    "# Linux client parameters\n",
    "if sys.platform.startswith('linux'):    \n",
    "    \n",
    "    # path to data\n",
    "    data_file_name = '/your-environment-path/to-model-data/imdb_master.csv'\n",
    "    \n",
    "    # path to HuggingFace cache directory\n",
    "    cache_dir='/your-environment-path/to-cache_dir'   \n",
    "    \n",
    "# Windows client parameters    \n",
    "elif sys.platform.startswith('win'):\n",
    "\n",
    "    # path to data\n",
    "    data_file_name = 'Drive:\\\\your-network-path\\\\to-model-data\\\\imdb_master.csv'\n",
    "    \n",
    "    # path to HuggingFace cache directory\n",
    "    cache_dir='Drive:\\\\your-path\\\\to-cache-dir'\n",
    "\n",
    "# Unsupported operating system client    \n",
    "else:\n",
    "    raise ValueError('Unrecognized operating system')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Connect-to-SAS-Viya-Server\"></a>\n",
    "<h3> Connect to the SAS Viya Server </h3>\n",
    "\n",
    "This section of code invokes the SAS Wrapper for Analytics Transfer (SWAT), configures the Python matplotlib utility for Jupyter notebook output display, connects to a SAS Viya server, and loads the SAS CAS DeepLearn action set into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Added action set 'deepLearn'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; actionset</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>deepLearn</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.00668s</span> &#183; <span class=\"cas-user\">user 0.00182s</span> &#183; <span class=\"cas-sys\">sys 0.00481s</span> &#183; <span class=\"cas-memory\">mem 0.205MB</span></small></p>"
      ],
      "text/plain": [
       "[actionset]\n",
       "\n",
       " 'deepLearn'\n",
       "\n",
       "+ Elapsed: 0.00668s, user: 0.00182s, sys: 0.00481s, mem: 0.205mb"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the Python matplotlib utility to display output in \n",
    "# Jupyter notebook cells.\n",
    "%matplotlib inline\n",
    "\n",
    "# The SAS Scripting Wrapper for Analytic Transfer (SWAT)\n",
    "# is a module used to connect to a SAS Viya server.\n",
    "from swat import * \n",
    "\n",
    "# Specify the name of your SAS Viya host\n",
    "host = 'your-host-name'\n",
    "\n",
    "# Specify your installation's port ID and UserID\n",
    "s = CAS(host, 99999, 'userID')\n",
    "\n",
    "# load the SAS Viya Deep Learning Action Set\n",
    "s.loadactionset('deepLearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Download-the-Data\"></a>\n",
    "<h3> Download the Raw Data </h3>\n",
    "\n",
    "In this section, you download the raw data that you will prepare for use with a BERT model.\n",
    "\n",
    "The model data in this example is the [Kaggle](http://kaggle.com) public-domain IMDB movie review data set named  [imdb_master.csv](https://www.kaggle.com/utathya/imdb-review-dataset). Download and store the ``imdb_master.csv`` file in a directory that your SAS Viya client can access. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data-Exploration-and-Cleaning\"></a>\n",
    "<h2>Explore and Clean the Data</h2>\n",
    "\n",
    "After downloading the ``imdb_master.csv`` file, examine the contents. You can use your favorite spreadsheet to open the CSV file, scan the contents, and see the data schema for this table. Each observation represents a movie review, along with its associated metadata.  \n",
    "\n",
    "While perusing the data, feel free to clean up any cell contents that might have formatting issues. For example, note that some of the movie review comments contain extraneous formatting tags (such as ``<br />``) that must be removed.  \n",
    "\n",
    "Note: If you don't clean the data while browsing the spreadsheet, it's OK. Before this example [extracts the training data](#Read-Data-into-Memory), a line of the example code searches for and removes all occurrences of the string ``<br />``. Afterwards, the code converts the format for target values from nominal to numeric. \n",
    "\n",
    "The following code reads the ``imdb_master.csv`` data for movie type, movie review text (input), movie sentiment label (target), and file name into a Pandas data frame.\n",
    "\n",
    "<a id=\"Read-Data-into-Memory\"></a>\n",
    "<h3>Read the Data into Pandas Data Frame</h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data set for IMDB movie review sentiment classification\n",
    "# into SAS CAS memory.\n",
    "reviews = pd.read_csv(data_file_name,\n",
    "                      header=0,\n",
    "                      names=['type', \n",
    "                             'review',\n",
    "                             'label',\n",
    "                             'file'\n",
    "                            ],\n",
    "                      encoding='latin_1'\n",
    "                     )\n",
    "\n",
    "# The input data is the text in the review column.\n",
    "input_label = 'review'       \n",
    "\n",
    "# The target data is the sentiment text in the label column.\n",
    "target_label = 'label'       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Extract-and-Shuffle-Data\"></a>\n",
    "<h3> Extract a Data Subset and Randomly Shuffle Observations</h3>\n",
    "\n",
    "The data contains three types of reviews: (**train**, **test**, and **unsup**).  This example only uses the  **train** reviews as it is a precursor step to traininig a BERT model. Hence, we must extract the correct subset of data.  \n",
    "\n",
    "We must also translate the raw data target variables from the nominal values *neg* and *pos* to consistent numeric values that are suitable for analysis.  We adopt a simple rule for the nominal-to-numeric translation:\n",
    "\n",
    "*neg* <=> 1  \n",
    "*pos* <=> 2\n",
    "\n",
    "**Note:** If you do not perform the nominal-to-numeric translation, your Viya data table will be incorrect.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import random function\n",
    "import random\n",
    "\n",
    "# Define inputs and targets\n",
    "t_idx1 = reviews['type'] == 'train'\n",
    "# Exclude observations where review type = 'unsup' \n",
    "t_idx2 = reviews['label'] != 'unsup'\n",
    "inputs = reviews[t_idx1 & t_idx2][input_label].to_list()\n",
    "targets = reviews[t_idx1 & t_idx2][target_label].to_list()\n",
    "\n",
    "# Nominal-to-numeric format change for target values\n",
    "for ii,val in enumerate(targets):\n",
    "    inputs[ii] = inputs[ii].replace(\"<br />\",\"\")\n",
    "    if val == 'neg':\n",
    "        targets[ii] = 1\n",
    "    elif val == 'pos':\n",
    "        targets[ii] = 2\n",
    "        \n",
    "# Shuffle the data\n",
    "# You must use this random seed value if you   \n",
    "# want to duplicate the example results.\n",
    "random.seed(123454321)\n",
    "map_index_position = list(zip(inputs, \n",
    "                              targets\n",
    "                              )\n",
    "                          )\n",
    "random.shuffle(map_index_position)\n",
    "inputs, targets = zip(*map_index_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data-Tokenization\"></a>\n",
    "<a id=\"Export-Data-to-Viya-Table\"></a>\n",
    "<h2> Tokenize the Data and Upload the Data as a SAS Viya Table </h2>\n",
    "\n",
    "After extracting and shuffling the input data, it must now be tokenized for use with BERT.\n",
    "\n",
    "The input data consists of text strings that are composed of English words, but BERT networks cannot operate directly on text data. Instead, the text must be tokenized using WordPiece tokens.  \n",
    "\n",
    "The WordPiece algorithm segments words into representative components, or tokens for NLP tasks. These WordPiece tokens are then translated to WordPiece embeddings internally by the BERT model. Embeddings are a vector of numeric values that represent the tokens. BERT combines the WordPiece embeddings with position and segment embeddings when forming the input to the first encoder layer.  \n",
    "\n",
    "A full discussion of these embeddings is beyond the scope of this example, but you can find a tutorial-style explanation [here](http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/).  \n",
    "\n",
    "After the input data has been tokenized, the table is ready to be uploaded as a SAS Viya table.\n",
    "\n",
    "The helper function ``bert_prepare_data()`` handles all of the data preparation and formatting that is required to upload the SAS Viya table.  In this example, the code splits the **train** data into training and validation data sets by specifying ``train_fraction=0.9``.  This code puts 90\\% of the data into the training data set, and assigns the remaining 10\\% to the validation data set.  \n",
    "\n",
    "The last parameter in the ``bert_prepare_data`` specification requests verbose output. This setting provides user feedback so you can track the progress of the data preparation. Be warned that some large data sets might take a long time to complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: 10% of the observations tokenized.\n",
      "NOTE: 20% of the observations tokenized.\n",
      "NOTE: 30% of the observations tokenized.\n",
      "NOTE: 40% of the observations tokenized.\n",
      "NOTE: 50% of the observations tokenized.\n",
      "NOTE: 60% of the observations tokenized.\n",
      "NOTE: 70% of the observations tokenized.\n",
      "NOTE: 80% of the observations tokenized.\n",
      "NOTE: 90% of the observations tokenized.\n",
      "NOTE: all observations tokenized.\n",
      "\n",
      "WARNING: 3329 out of 25000 observations exceeded the maximum sequence length\n",
      "These observations have been truncated so that only the first 512 tokens are used.\n",
      "\n",
      "NOTE: uploading training data to CAS table bert_train_data.\n",
      "NOTE: there are 22593 observations in the training data set.\n",
      "\n",
      "NOTE: uploading test/validation data to CAS table bert_test_validation_data.\n",
      "NOTE: there are 2407 observations in the test/validation data set.\n",
      "\n",
      "NOTE: training and test/validation data sets ready.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT data preparation helper function\n",
    "from dlpy.transformers.bert_utils import bert_prepare_data\n",
    "\n",
    "# Run the BERT data preparation helper function\n",
    "num_tgt_var, train, valid = bert_prepare_data(s, \n",
    "                                              BertTokenizer.from_pretrained('bert-base-uncased',\n",
    "                                                                            cache_dir=cache_dir\n",
    "                                                                           ), \n",
    "                                              512, \n",
    "                                              input_a=list(inputs), \n",
    "                                              target=list(targets), \n",
    "                                              classification_problem=True,\n",
    "                                              segment_vocab_size=2,\n",
    "                                              \n",
    "                                              # Partition the extracted data  \n",
    "                                              # into 90% Train partition and \n",
    "                                              # 10% Validation partition.                                            \n",
    "                                              train_fraction=0.9,\n",
    "                                              \n",
    "                                              # Verbose user feedback\n",
    "                                              verbose=True\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Review and Verify the Newly Prepared BERT Data </h2>\n",
    "\n",
    "According to the output, the example should have created new train and validation data tables. Now we can review the new tables and verify that they have the expected number and type of columns, headings, and cell contents. After verifying proper column structures and contents in the new tables, you can also display the summary statistics to get a sense of the data.\n",
    "\n",
    "* [Verify Training Table Columns](#Verify-Train-Columns)\n",
    "* [Verify Validation Table Columns](#Verify-Valid-Columns)\n",
    "* [Review Training Data](#Review-Train-Data)\n",
    "* [Display Training Summary Statistics](#Display-Train-Summary)\n",
    "* [Review Validation Data](#Review-Valid-Data)\n",
    "* [Display Validation Summary Statistics](#Display-Valid-Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Verify-Train-Columns\"></a>\n",
    "<h3> Verify Training Table Columns </h3>\n",
    "\n",
    "The ``bert_prepare_data`` function creates variables in the Viya table that are used by the token, position, and segment embeddings.  If you are using data that has associated labels, the function also creates target and target length variables.  Since this example uses labeled data, the output of the *columinfo* action in the following code should contain at least 5 columns:\n",
    "\n",
    "* \\_tokens\\_         => tokenized input text strings\n",
    "* \\_position\\_       => position indication strings used by position embeddings\n",
    "* \\_segment\\_        => segment indication strings used by segment embeddings\n",
    "* \\_target\\_0\\_      => target variable (only one for this example)\n",
    "* \\_target\\_length\\_ => target length variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ColumnInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Column\">Column</th>\n",
       "      <th title=\"Label\">Label</th>\n",
       "      <th title=\"ID\">ID</th>\n",
       "      <th title=\"Type\">Type</th>\n",
       "      <th title=\"RawLength\">RawLength</th>\n",
       "      <th title=\"FormattedLength\">FormattedLength</th>\n",
       "      <th title=\"Format\">Format</th>\n",
       "      <th title=\"NFL\">NFL</th>\n",
       "      <th title=\"NFD\">NFD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_tokens_</td>\n",
       "      <td>_tokens_</td>\n",
       "      <td>1</td>\n",
       "      <td>varchar</td>\n",
       "      <td>2893</td>\n",
       "      <td>2893</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_position_</td>\n",
       "      <td>_position_</td>\n",
       "      <td>2</td>\n",
       "      <td>varchar</td>\n",
       "      <td>6545</td>\n",
       "      <td>6545</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_segment_</td>\n",
       "      <td>_segment_</td>\n",
       "      <td>3</td>\n",
       "      <td>varchar</td>\n",
       "      <td>5631</td>\n",
       "      <td>5631</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_target_0_</td>\n",
       "      <td>_target_0_</td>\n",
       "      <td>4</td>\n",
       "      <td>varchar</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_target_length_</td>\n",
       "      <td>_target_length_</td>\n",
       "      <td>5</td>\n",
       "      <td>double</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.00167s</span> &#183; <span class=\"cas-user\">user 0.00125s</span> &#183; <span class=\"cas-sys\">sys 0.00037s</span> &#183; <span class=\"cas-memory\">mem 0.806MB</span></small></p>"
      ],
      "text/plain": [
       "[ColumnInfo]\n",
       "\n",
       "             Column            Label  ID     Type  RawLength  FormattedLength  \\\n",
       " 0         _tokens_         _tokens_   1  varchar       2893             2893   \n",
       " 1       _position_       _position_   2  varchar       6545             6545   \n",
       " 2        _segment_        _segment_   3  varchar       5631             5631   \n",
       " 3       _target_0_       _target_0_   4  varchar          1                1   \n",
       " 4  _target_length_  _target_length_   5   double          8               12   \n",
       " \n",
       "   Format  NFL  NFD  \n",
       " 0           0    0  \n",
       " 1           0    0  \n",
       " 2           0    0  \n",
       " 3           0    0  \n",
       " 4           0    0  \n",
       "\n",
       "+ Elapsed: 0.00167s, user: 0.00125s, sys: 0.00037s, mem: 0.806mb"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.columninfo(table=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Verify-Valid-Columns\"></a>\n",
    "<h3> Verify Validation Table Columns </h3>\n",
    "\n",
    "As with the training data, The ``bert_prepare_data`` function creates variables in the validation table that are used by the token, position, and segment embeddings.  If you are using data that has associated labels, the function also creates target and target length variables.  Since this example uses labeled data, the output of the *columinfo* action in the following code should contain at least 5 columns:\n",
    "\n",
    "* \\_tokens\\_\n",
    "* \\_position\\_\n",
    "* \\_segment\\_\n",
    "* \\_target\\_0\\_\n",
    "* \\_target\\_length\\_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ColumnInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Column\">Column</th>\n",
       "      <th title=\"Label\">Label</th>\n",
       "      <th title=\"ID\">ID</th>\n",
       "      <th title=\"Type\">Type</th>\n",
       "      <th title=\"RawLength\">RawLength</th>\n",
       "      <th title=\"FormattedLength\">FormattedLength</th>\n",
       "      <th title=\"Format\">Format</th>\n",
       "      <th title=\"NFL\">NFL</th>\n",
       "      <th title=\"NFD\">NFD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_tokens_</td>\n",
       "      <td>_tokens_</td>\n",
       "      <td>1</td>\n",
       "      <td>varchar</td>\n",
       "      <td>2857</td>\n",
       "      <td>2857</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_position_</td>\n",
       "      <td>_position_</td>\n",
       "      <td>2</td>\n",
       "      <td>varchar</td>\n",
       "      <td>6545</td>\n",
       "      <td>6545</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_segment_</td>\n",
       "      <td>_segment_</td>\n",
       "      <td>3</td>\n",
       "      <td>varchar</td>\n",
       "      <td>5631</td>\n",
       "      <td>5631</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_target_0_</td>\n",
       "      <td>_target_0_</td>\n",
       "      <td>4</td>\n",
       "      <td>varchar</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_target_length_</td>\n",
       "      <td>_target_length_</td>\n",
       "      <td>5</td>\n",
       "      <td>double</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.000828s</span> &#183; <span class=\"cas-user\">user 0.000796s</span> &#183; <span class=\"cas-memory\">mem 0.806MB</span></small></p>"
      ],
      "text/plain": [
       "[ColumnInfo]\n",
       "\n",
       "             Column            Label  ID     Type  RawLength  FormattedLength  \\\n",
       " 0         _tokens_         _tokens_   1  varchar       2857             2857   \n",
       " 1       _position_       _position_   2  varchar       6545             6545   \n",
       " 2        _segment_        _segment_   3  varchar       5631             5631   \n",
       " 3       _target_0_       _target_0_   4  varchar          1                1   \n",
       " 4  _target_length_  _target_length_   5   double          8               12   \n",
       " \n",
       "   Format  NFL  NFD  \n",
       " 0           0    0  \n",
       " 1           0    0  \n",
       " 2           0    0  \n",
       " 3           0    0  \n",
       " 4           0    0  \n",
       "\n",
       "+ Elapsed: 0.000828s, user: 0.000796s, mem: 0.806mb"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.columninfo(table=valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Review-Train-Data\"></a>\n",
    "<h3> Review Training Data </h3>\n",
    "\n",
    "This step is a good development practice, but is not strictly necessary. It is usually a good idea to review your pre-processed data in order to verify that there are no remaining artifacts.  The code and output cell below displays the token and target column values for a small number of random observations, so you can visually inspect them.  You can control what is displayed through the ``num_obs`` and ``columns`` arguments.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Observation:  18302 -------\n",
      "\n",
      "_tokens_ :  [CLS] this movie describes the life of somebody who grew up in the worst of circumstances but unlike many people he actually grew up to be a respectable person . what ##s more is that this is a true story . ant ##won ##e fisher is so innocent and yet he was abused such just because he was not white . ant ##won ##e fisher has been married to the same women for ten years and he never fooled around with women , coke , cigar ##s , weed , alcohol , or any of those things that are very popular in the places he was growing up . there is not much more to say about this movie it is excellent . the only rating i can give it is a 10 / 10 . [SEP]\n",
      "\n",
      "\n",
      "_target_0_ :  2\n",
      "\n",
      "\n",
      "------- Observation:  2230 -------\n",
      "\n",
      "_tokens_ :  [CLS] this is a truly awful [\"] b [\"] movie . it is wit ##less and often embarrassing . the plot , the basic [\"] making into show business [\"] routine , is almost none ##xi ##sten ##t . in fact , the film is merely an excuse to push the war effort and highlight some popular music groups of 1942 , including the mills brothers , count bas ##ie , duke ellington , bob crosby , and freddy slack . each group gets about the standard three minutes , the exception being the mills brothers , who for some reason warrant ##ed two numbers . ann miller doesn ['] t get to dance until the last couple of minutes of the film , and she has little to do but st ##rut her stuff amid a barrage of patriotic propaganda . the most interesting moment in the film , in my view , occurred in the duke ellington segment . the band appears to be playing in a train , standing in awkward positions . ( in the deep south at the time , the band was segregated in railroad cars when traveling . ) johnny hodges is seen next to duke , and harry car ##ney may also be identified . in the last moments of the film , trumpeter / violinist ray nan ##ce rushes down the aisle to the camera and does an [\"] uncle tom , [\"] bug ##ging his eyes and wig ##gling his head the way willy best did in many films . for modern viewers , especially jazz fans , this homage to segregation is sad indeed . some movies go best unseen . [SEP]\n",
      "\n",
      "\n",
      "_target_0_ :  1\n",
      "\n",
      "\n",
      "------- Observation:  8532 -------\n",
      "\n",
      "_tokens_ :  [CLS] [\"] a guy thing [\"] may not be a classic , but it sure is a good , funny comedy . the plot focuses on paul ( jason lee ) , who wakes up the morning after his bachelor party with no memory and becky ( julia stil ##es ) lying naked in his bed . before he can figure out what happened , he rushes becky out of his apartment because his fiance karen ( selma blair ) is coming . after that , as you could imagine , chaos en ##su ##es . almost every single scene in [\"] a guy thing [\"] delivers loud laughs . the fun ##nies ##t moments come from when paul imagine ##s what could happen if he tells karen . selma blair is a truly talented comedian , and the worst thing about this film is that she goes under ##used . although , she turns out to be more funny than stil ##es ['] character , who actually isn ['] t that interesting . of course , not every comedy is perfect . as i said , [\"] a guy thing [\"] is no classic , but it ['] s not bad either , 7 / 10 . [SEP]\n",
      "\n",
      "\n",
      "_target_0_ :  2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import DLPy BERT display observations function\n",
    "from dlpy.transformers.bert_utils import display_obs\n",
    "\n",
    "# Display the token and target contents for three \n",
    "# observations from the training data\n",
    "display_obs(s, train, num_obs=3, columns=['_tokens_', '_target_0_'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"Display-Train-Summary\"></a>\n",
    "<h3> Display Training Data Summary Statistics </h3>\n",
    "\n",
    "This step is also a good development practice, but is not strictly necessary. The ``summary()`` function provides an idea of how the input data is distributed in terms of the number of input tokens.  The code below specifies ``full_table=False`` in order to base the statistics on a subset of the full table.  Using a table subset might be necessary with very large tables because otherwise the full table must be copied from the SAS Viya server to the client to compute summary statistics. \n",
    "\n",
    "**Note:** You should expect variations between summary statistics that are generated using a data subset and summary statistics that are generated using the full data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: there are 22593 observations in the training data set.\n",
      "NOTE: calculating summary statistics based on the first 10% of the table.\n",
      "\n",
      "NOTE: minimum number of tokens in an observation = 31\n",
      "NOTE: maximum number of tokens in an observation = 512\n",
      "NOTE: average number of tokens in an observation = 270.2421425409473\n",
      "NOTE: standard deviation of the number of tokens in an observation = 138.61731127606353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import DLPy BERT Summary Statistics Function\n",
    "from dlpy.transformers.bert_utils import bert_summary\n",
    "\n",
    "# Display training data summary statistics \n",
    "# using a subset of the full table\n",
    "bert_summary(s, train, full_table=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Review-Valid-Data\"></a>\n",
    "<h3> Review Validation Data </h3>\n",
    "\n",
    "The code and output cell below displays random columns for a small number of observations.  The previous invocation of ``display_obs()`` in the [Review Training Data](#Review-Train-Data) code displayed only two columns (the token and target values), but you can specify as many columns as you want.  \n",
    "\n",
    "The only drawback of specifying multiple specific columns is that the returned output can be excessive.  Selecting random column output (the default) allows you to spot check all columns with a manageable amount of output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Observation:  380 -------\n",
      "\n",
      "_tokens_ :  [CLS] this review is mostly all spoil ##ers . if you plan on enjoying this film , don ['] t read this review . that ['] s the problem with kids tv nowadays . it ['] s all so patron ##izing and conde ##sc ##ending . ` wow , that was fun , wasn ['] t it ? ['] no it wasn ['] t . and unfortunately it seems to have per ##me ##ated into children films as well . and that is what ['] flight of the reindeer ['] is all about . admitted ##ly i haven ['] t seen ['] flight of the reindeer ['] in a few years so i might be hazy on some points , but i remember being thoroughly un ##im ##pressed with it at the time . essentially , the story follows a lecturer who is given a book for christmas . now , the lecturer is an esteem ##ed scientist on the flying habits of some animal . i think it was bull ##fr ##og ##s . anyway , through this book , mr lecturer / family man learns that reindeer can fly in exactly the same way as bull ##fr ##og ##s . apparently this book was written by a scientist many hundreds of years ago who disappeared in the north pole . now , if it were me i would have thrown the book out the nearest window . flying bull ##fr ##og ##s are a naturally occurring phenomenon , but flying reindeer is a fantastic and wholly un ##real ##istic concept . but , mr lecturer isn ['] t me , so i guess that explains why he decides to fly to the north pole leaving his wife and kid at home with no idea where he ['] s gone . of course , things go aw ##ry and before he knows it , a flying reindeer has crashed into his private plane and he ['] s stranded at the north pole . are you still following this ? good . his family , of course , are devastated . i would be too , i mean , what a suck - ta ##cular christmas . elsewhere , mr lecturer finds himself in a hidden town at the north pole inhabited by mid ##get ##s and one ty ##ran ##nical figure who dresses only in red and white . rather than try to escape immediately , as i would have done , he greet ##s everyone there with open arms . this hidden civilization of mid ##get monsters , and he greet ##s them with open arms . o ##oo ##o - kay ##y ##y ##y . they feed him this story that they are the elves of santa claus and they spend all year round making toys for kids they have no idea exist . and mr lecturer accepts all this . he even accepts that santa claus is in fact the scientist who disappeared two hundred years ago or whatever . there ['] s just one problem - [SEP]\n",
      "\n",
      "\n",
      "------- Observation:  2338 -------\n",
      "\n",
      "_tokens_ :  [CLS] another detailed work on the subject by dr d ##wi ##ved ##i takes us back in time to pre - part ##ioned pan ##ja ##b . dr d ##wi ##ved ##i chose a difficult subject for his movie debut . he has worked on all met ##ic ##ulous details to bring the story to life . the treatment of the subject is very delicate . even though we have not been to the region during that time , the sets and costumes look real . unlike most movies made on partition , this one focuses not on the go ##ry details of violence to attract audience , but on its after - effects . the characters come to life . pri ##yan ##shu chatter ##jee has given an impressive performance . man ##oj ba ##j ##pa ##i has acted his heart out showing the plight of a guilt - ridden man . the rest of the cast has done a good job too . [SEP]\n",
      "\n",
      "\n",
      "------- Observation:  415 -------\n",
      "\n",
      "_position_ :  position_0 position_1 position_2 position_3 position_4 position_5 position_6 position_7 position_8 position_9 position_10 position_11 position_12 position_13 position_14 position_15 position_16 position_17 position_18 position_19 position_20 position_21 position_22 position_23 position_24 position_25 position_26 position_27 position_28 position_29 position_30 position_31 position_32 position_33 position_34 position_35 position_36 position_37 position_38 position_39 position_40 position_41 position_42 position_43 position_44 position_45 position_46 position_47 position_48 position_49 position_50 position_51 position_52 position_53 position_54 position_55 position_56 position_57 position_58 position_59\n",
      "\n",
      "\n",
      "------- Observation:  13 -------\n",
      "\n",
      "_tokens_ :  [CLS] like most people , i ['] ve seen jason priest ##ley on tv and i think he ['] s great . but i didn ['] t know he had a sister ! justine priest ##ley is simply [\"] ma ##h [\"] ve ##lous as the sc ##orne ##d other woman . good music , int ##rigue , and a death scene involving amanda ['] s revenge on an abusive dr . that will stay with you for weeks . i ['] ll leave it at that . ku ##dos . [SEP]\n",
      "\n",
      "\n",
      "------- Observation:  958 -------\n",
      "\n",
      "_segment_ :  segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0 segement_0\n",
      "\n",
      "\n",
      "------- Observation:  946 -------\n",
      "\n",
      "_tokens_ :  [CLS] this was a great romantic comedy ! historically inaccurate ( einstein had no niece ##s or nephew ##s as noted elsewhere in im ##db ) but he made a great match ##maker . he brought together two very nice people played by two of the best actors working . the supporting cast ( lou jacob ##i , joseph ma ##her , gene sa ##ks , stephen fry , etc . ) all clicked on screen and made this a great viewing experience . the fact he drove a car to get around seems to contra ##dict all those images and posters of him riding a bicycle to get around . and did he wear socks in one scene . . . reportedly , the professor never wore socks ! ( two new entries for the im ##db goo ##fs section . ) historical ina ##cc ##ura ##cies and inc ##ons ##iste ##ncies aside , this was a great movie to watch with a great cast . i give it an 8 ! [SEP]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display random columns from six \n",
    "# randomly selected observations\n",
    "display_obs(s, valid, num_obs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Display-Valid-Summary\"></a>\n",
    "<h3> Display Validation Data Summary Statistics </h3>\n",
    "\n",
    "Here we compute summary statistics for the validation data. We use the full validation table (instead of using a subset) because it is significantly smaller in size than the training table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: there are 2407 observations in the training data set.\n",
      "NOTE: minimum number of tokens in an observation = 35\n",
      "NOTE: maximum number of tokens in an observation = 512\n",
      "NOTE: average number of tokens in an observation = 268.1312837557125\n",
      "NOTE: standard deviation of the number of tokens in an observation = 138.70136390205272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display summary statistics for the validation data.\n",
    "# This is a small table, so it is not necessary to \n",
    "# subset the validation data for summary reporting.\n",
    "bert_summary(s, valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the training and validation data look good and are ready for use with a BERT model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
